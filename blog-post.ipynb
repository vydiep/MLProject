{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composer Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract (Vy)\n",
    "\n",
    "In this project, we will work to identify a composer of classical music. More specifically, we will identify whether a piece of classical music is Beethoven or not. In order to accomplish this, we will be using a data set with audio files of songs from different composers and metadata about those songs. Through strategic feature selection and potentially deep learning, we hope to yield successful results, in that our model is able to correctly classify whether a piece of music is Beethoven's at least 80-85% of the time given that there's a labeling error rate of 4% in our data set. (edit this)\n",
    "\n",
    "GitHub Repository: [MLProject](https://github.com/vydiep/MLProject)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When listening to music, it's common to encounter an unfamiliar song that one wishes to quickly identify. To address this challenge, our project leverages the MusicNet dataset to classify classical music and classify whether a given piece is composed by Beethoven. This project aimed to tackle this seemingly small but persistent issue of identifying music, with the hope of contributing to the larger problem of identifying unknown music, which is beyond our current capabilities.  \n",
    "\n",
    "Since composers may experiment and deviate from their established patterns in their music, there is no guarentee that there will be patterns in artists music audio files, thus aren't many resources on how others within the Machine Learning field addressed this problem. On the other hand, genre classification using music audio files is a widely explored problem with numerous approaches available. @costa2017evaluation used the ISMIR 2004 Database, a western music collection, the LMD database, a collection of Latin American music, and a collection of field recordings of ethnic African music to train a model to classify music genres. They found that the spectrograms of audio when used with a convolutional neural network performs just as well if not better than individual classifiers trained with visual representation on all datasets. @khamees2021classifying used the GTZAN dataset for music classification. In their studies, they created and compared the performance of CNN and RNN models. They found that CNN with Max-Pooling outperformed RNN with LSTM, yielding a testing accuracy of 74%, although it took longer. @kakarla2022recurrent also used the GTZAN dataset, worked with the MFCCs of audio files, and found that a 5-layed stacked independent RNN was able to achieve 84% accuracy. \n",
    "\n",
    "While our project may not be as complex as music genre classification, we used this research as guidance and created a variety of simple CNNs and RNNs with LSTM to work towards this problem of classifying the composer of classical music. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values Statement\n",
    "\n",
    "As avid music listeners, we were drawn to the idea of creating a project focused on analyzing music using its audio files. Our project, which works to classify a piece of classical music as Beethoven or not, could potentially be used by music theory researchers, classical music listeners, and anyone interested in exploring this genre. If we kept the labels indicating the original composer rather than changing it to \"Other,\" we could potentially identify patterns and influences among different composers. It's also important to note that our models were only trained on classical music from Western composers, excluding non-Western composers and potentially perpetuate inequalities in the music industry. Misclassifications could also result in the improper attribution of credit to certain composers. While our project is still small in scale, we recognize that as it grows, it may require additional computational resources, which could introduce environmental concerns.  \n",
    "\n",
    "While our project can do a better job of incorporating classical music from non-Western composers, we still believe that it could bring joy and value to the world of classical music. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and Methods \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data (Vy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach (Vy)\n",
    "\n",
    "How you trained your models, and on what hardware.\n",
    "How you evaluated your models (loss, accuracy, etc), and the size of your test set.\n",
    "Whether you subset your data in any way, and for what reasons.\n",
    "\n",
    "#### CNN\n",
    "\n",
    "\n",
    "#### RNN \n",
    "Another model we used to analyze our clips of music was a recursive neural network (RNN) with long short-term memory (LSTM) architecture, which are good at understanding order and analyzing sequential data. We utilized the mel-frequency cepstral coefficients (MFCC) of our audio clips as the input features. MFCCs are commonly used frequency domain features that represent the frequencies perceived by the human ear and capture the \"brightness\" of sound. For this model, we also used our composer labels as targets. Since our task was fairly simple, we trained four variations of simple LSTM models. We first experimented with 1 LSTM layer and 2 LSTM layers. Then to prevent overfitting, we experimented with 1 LSTM layer with a dropout layer and 2 LSTM layers with a dropout layer, bother with a of probability of 0.2. \n",
    "\n",
    "\n",
    "What features of your data you used as predictors for your models, and what features (if any) you used as targets.\n",
    "What model(s) you used trained on your data, and how you chose them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "### CNN\n",
    "\n",
    "### RNN\n",
    "Below are the history of our training and validation accuracy for our RNN models with 1 LSTM layer, 2 LSTM layers, 1 LSTM layer and a dropout layer, and 2 LSTM layers and a dropout layer.  \n",
    "![Graph of training and validation accuracy and loss for RNN model with 1 LSTM layer](https://github.com/vydiep/MLProject/blob/main/RNN/Models/LSTM/LSTM-1-Layer-graph.png?raw=true)  \n",
    "![Graph of training and validation accuracy and loss for RNN model with 2 LSTM layers](https://github.com/vydiep/MLProject/blob/main/RNN/Models/LSTM/LSTM-2-Layers-graph.png?raw=true)  \n",
    "![Graph of training and validation accuracy and loss for RNN model with 1 LSTM layer and Dropout layer](https://github.com/vydiep/MLProject/blob/main/RNN/Models/LSTM-Dropout/LSTM-1-Layer-Dropout-graph.png?raw=true)  \n",
    "![Graph of training and validation accuracy and loss for RNN model with 2 LSTM layers and Dropout layer](https://github.com/vydiep/MLProject/blob/main/RNN/Models/LSTM-Dropout/LSTM-2-Layers-Dropout-graph.png?raw=true)\n",
    "As conveyed in the graphs, we trained for about 100 epochs, for that's when it looked like the training validation accuracy started to plateau, and were never able to surpass a training validation accuracy of 80%. For our model with 1 LSTM layer, we achieved a testing accuracy of 74%. For our model with 2 LSTM layers, we achieved a testing accuracy of 78%. For our model with 1 LSTM layer and a dropout layer, we achieved a testing accuracy of 76%. Lastly, for our model with 2 LSTM layers and a dropout we achieved a testing accuracy of 76%. While these accuracies are not the best, they are better than randomly guessing given that 48% of our data is Beethoven. \n",
    "\n",
    "We would also like to acknowledge that our results are based on one round of training on our data. In order to achieve more accurate results, we understand that training our models multiple times and averaging their results would provide a more accurate representation of how our models do. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Discussion\n",
    "\n",
    "In what ways did our project work?\n",
    "Did we meet the goals that we set at the beginning of the project?\n",
    "How do our results compare to the results of others who have also studied similar problems?\n",
    "If we had more time, data, or computational resources, what might we do differently in order to improve further?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Contribution Statement \n",
    "\n",
    "The work for this project was fairly evenly distributed.  \n",
    "\n",
    "Vy Diep wrote the code for the data prepartion and the CNN model training/experimentation (maybe add more context). For our presentation, she led the discussion on our approach and CNN results. For the blog post, she led the writing for the Abstract, Data, Approach, and CNN results sections. \n",
    "\n",
    "Katie Macalintal wrote the code for the RNN model training/experimentation. For our presentation, she led the discussion on the overview of our project and RNN results. For the blog post, she led the writing for the Introduction, Values Statement, and RNN results sections. \n",
    "\n",
    "There were also a handful of tasks that we'd do together, but only one person would commit. We would often clean up our GitHub repository together and for the blog post, we worked together to craft the concluding discussion. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Reflections\n",
    "\n",
    "What did you learn from the process of researching, implementing, and communicating about your project?\n",
    "How do you feel about what you achieved? Did meet your initial goals? Did you exceed them or fall short? In what ways?\n",
    "In what ways will you carry the experience of working on this project into your next courses, career stages, or personal life?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
